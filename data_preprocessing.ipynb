{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_preprocessing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNRmj81911G0NYguN9op2Y6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import re\n","from google.colab import drive\n","import string\n","import pickle\n","import numpy as np\n","import gensim\n","\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","\n","from keras.utils import np_utils\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","\n","from numpy.core.fromnumeric import shape\n","from keras.layers import Dense, Input, Dropout, Flatten, Embedding, CuDNNLSTM, LSTM\n","from keras.models import Sequential"],"metadata":{"id":"tV_2_MN0vWiy","executionInfo":{"status":"ok","timestamp":1646781484047,"user_tz":-120,"elapsed":6299,"user":{"displayName":"Mahmoud mohey el-din","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13699406433670243851"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["drive.mount('/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCOebfC0-XzA","executionInfo":{"status":"ok","timestamp":1646781502665,"user_tz":-120,"elapsed":18629,"user":{"displayName":"Mahmoud mohey el-din","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13699406433670243851"}},"outputId":"2c0a89aa-6360-4fd7-8d7e-45fe76202489"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":["DATASET_PATH = '/gdrive/MyDrive/text_dialect_dataset.pkl'"],"metadata":{"id":"aFfnMGrBH1C4","executionInfo":{"status":"ok","timestamp":1646781502667,"user_tz":-120,"elapsed":12,"user":{"displayName":"Mahmoud mohey el-din","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13699406433670243851"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class WORD2VECTOR:\n","  def __init__(self, vector_size, iterations):\n","    # Size of vector that represents the word\n","    self.vector_size = vector_size\n","    self.iterations = iterations\n","    self.word2vec_model = None\n","    self.w2v_dict = None\n","    self.num_of_unique_words = None\n","    self.gensim_weight_matrix = None\n","\n","  # Build word2vec model\n","  def create_model(self, data):\n","    self.word2vec_model = gensim.models.Word2Vec(data, min_count=3, size=self.vector_size, iter=self.iterations)\n","    self.initialize_w2v_properties()\n","    return self.word2vec_model\n","  \n","  # Create dict of words and their weights\n","  def initialize_w2v_properties(self):\n","    self.w2v_dict = dict(zip(self.word2vec_model.wv.index2word, self.word2vec_model.wv.syn0))\n","    # Number of unique words in model\n","    self.num_of_unique_words = len(self.word2vec_model.wv.vocab)\n","\n","  # Create Embedding matrix for words\n","  def create_embedding_matrix(self, tokenizer):\n","    self.gensim_weight_matrix = np.zeros((self.num_of_unique_words ,self.vector_size))\n","    for word, index in tokenizer.word_index.items():\n","      if index < self.num_of_unique_words - 1:  \n","        if word in self.word2vec_model.wv.vocab:\n","          self.gensim_weight_matrix[index] = self.word2vec_model[word]\n","        else:\n","          self.gensim_weight_matrix[index] = np.zeros(300)\n","    return self.gensim_weight_matrix\n","  \n","  # Get mean of each sentence to have one vector of N dimension representing the sentence\n","  def transform(self, data):\n","    return np.array([\n","      np.mean([self.w2v_dict[w] for w in words if w in self.w2v_dict] or [np.zeros(self.vector_size)], axis=0) for words in data\n","    ])\n","\n","  def save_model(self, path, model):\n","    model.save(path)\n","  \n","  def load_model(self, path):\n","    self.word2vec_model = gensim.models.Word2Vec.load(path)\n","    self.initialize_w2v_properties()\n","    return self.word2vec_model"],"metadata":{"id":"BOUjZ17utCwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DataPreProcessing:\n","  NON_ARABIC_PATTERN = r'[^؀-ۿ]+'\n","  # Just incase we have arabian username\n","  MENTION_PATTERN = r'@\\S+'\n","  HASHTAG_PATTERN = r'#\\S+'\n","  # Normalize repeating\n","  REPEATING_CHARACTERS = r'(.)\\1+'\n","  \n","  def __init__(self):\n","    self.tokenizer = None\n","  \n","  # Apply the regular expressions on string and strip it\n","  def preprocess_string(self, text):\n","    text = re.sub(self.HASHTAG_PATTERN, ' ', text)\n","    text = re.sub(self.MENTION_PATTERN, ' ',  text)\n","    text = re.sub(self.NON_ARABIC_PATTERN, ' ', text)\n","    text = re.sub(self.REPEATING_CHARACTERS, r'\\1', text)\n","    #text = normalize.normalize_searchtext(text)\n","    processed_text = text.strip()\n","    return processed_text\n","\n","  # Tokenize sentence by word level\n","  def tokenize_sentence(self, sentence):\n","    return sentence.split()\n","  \n","  # Create pad tokenizer to have same vector length for each word \n","  def create_padding_tokenizer(self, num_of_unique_words):\n","    self.tokenizer = Tokenizer(num_of_unique_words)\n","    return self.tokenizer\n","\n","  def pad_data(self, data, longest_sequence, tokenizer):\n","    data = tokenizer.texts_to_sequences(data) # this converts texts into some numeric sequences \n","    data_pad = pad_sequences(data, maxlen=longest_sequence, padding='post') # this makes the length of all numeric sequences equal\n","    return data_pad"],"metadata":{"id":"3l-q5ZKxI-ca","executionInfo":{"status":"ok","timestamp":1646781727201,"user_tz":-120,"elapsed":305,"user":{"displayName":"Mahmoud mohey el-din","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13699406433670243851"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def run_processing():\n","  # Load dataset\n","  with open(DATASET_PATH, 'rb') as f:\n","    dialect_df = pickle.load(f)\n","  \n","  # First let's check for nulls and data types\n","  print(dialect_df.info())\n","  print('---------------------------\\n')\n","\n","  # Check for duplicates\n","  print('Duplicates =', dialect_df.duplicated().sum())\n","  print('---------------------------\\n')\n","\n","  # Check for class count\n","  print(dialect_df['dialect'].value_counts())\n","  print('---------------------------')\n","  \n","  # Load Preprocesing class\n","  dp = DataPreProcessing()\n","\n","  # Replacing old string with the processed one\n","  dialect_df['text'] = dialect_df['text'].apply(dp.preprocess_string)\n","\n","  # Add new column for the tokenized string in dataframe\n","  dialect_df['tokenized_string'] = dialect_df['text'].apply(dp.tokenize_sentence)\n","\n","  # Separate Feature from label\n","  X = dialect_df['text'].values\n","\n","  # Build vectorizer\n","  vectorizer = TfidfVectorizer(min_df=4, max_df=.25)\n","  vectorizer.fit(X)\n"," \n","  # Build word2vec model\n","  X = dialect_df['tokenized_string']\n","  wv = WORD2VECTOR(300, 100)\n","  #wv_model = wv.create_model(X)\n","  wv_model = wv.load_model('/gdrive/MyDrive/model4.bin')\n","\n","  # Build tokenizer for padding\n","  longest_sequence = max(X.apply(len)) # Longest sentence\n","  X = dialect_df['text']\n","  unique_words_count = wv.num_of_unique_words # Number of uniquer words in vocab\n","  tokenizer = dp.create_padding_tokenizer(unique_words_count)\n","  tokenizer.fit_on_texts(X)\n","  \n","  # Create embedding matrix\n","  embed_matrix = wv.create_embedding_matrix(tokenizer)\n","\n","  # Saving some stuff for further use\n","  # Save New dataframe\n","  #with open('/gdrive/MyDrive/processed_dialect_dataset.pkl', 'wb') as f:\n","  #  pickle.dump(dialect_df, f)\n","  \n","  # Save vecotrizer\n","  #with open('/gdrive/MyDrive/tfidf_vecctorizer.pkl', 'wb') as f:\n","  #  pickle.dump(vectorizer, f)\n","  \n","  # Save word2vec model\n","  #wv.save_model('/gdrive/MyDrive/model4.bin', wv_model)\n","\n","  # Save tokenizer\n","  #with open('/gdrive/MyDrive/tokenizer.pkl', 'wb') as f:\n","  #  pickle.dump(tokenizer, f)\n","  \n","  # Save embedding matrix\n","  #with open('/gdrive/MyDrive/embedding_matrix.pkl', 'wb') as f:\n","  #  pickle.dump(embed_matrix, f)"],"metadata":{"id":"NKHTxfrAEnSN","executionInfo":{"status":"ok","timestamp":1646781842876,"user_tz":-120,"elapsed":307,"user":{"displayName":"Mahmoud mohey el-din","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13699406433670243851"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["#run_processing()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"caUdXRPnLwwW","executionInfo":{"status":"ok","timestamp":1646686900694,"user_tz":-120,"elapsed":58544,"user":{"displayName":"Mahmoud mohey el-din","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13699406433670243851"}},"outputId":"19b44248-f896-4e98-c3e7-7893e8f0836d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 458197 entries, 0 to 458196\n","Data columns (total 3 columns):\n"," #   Column   Non-Null Count   Dtype \n","---  ------   --------------   ----- \n"," 0   id       458197 non-null  object\n"," 1   dialect  458197 non-null  object\n"," 2   text     458197 non-null  object\n","dtypes: object(3)\n","memory usage: 10.5+ MB\n","None\n","---------------------------\n","\n","Duplicates = 0\n","---------------------------\n","\n","EG    57636\n","PL    43742\n","KW    42109\n","LY    36499\n","QA    31069\n","JO    27921\n","LB    27617\n","SA    26832\n","AE    26296\n","BH    26292\n","OM    19116\n","SY    16242\n","DZ    16183\n","IQ    15497\n","SD    14434\n","MA    11539\n","YE     9927\n","TN     9246\n","Name: dialect, dtype: int64\n","---------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n","  app.launch_new_instance()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"]}]}]}